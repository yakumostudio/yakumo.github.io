<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title>Neural Head Avatars from Monocular RGB Videos</title>
        <link rel="stylesheet" href="../w3.css">
        <link rel="stylesheet"
              href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
              integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk"
              crossorigin="anonymous">
        <meta name="keywords" content="audio-visual speech recognition, self-supervised, multi-view representations, modality-missing, paper, research"/>
    </head>
    <body>
        <br/>
        <br/>
        <div class="w3-container" id="paper">
            <div class="w3-content" style="max-width:850px">
                <h2 align="center" id="title"><b>Enhanced Self-Supervised Multi-View Representations With Modality-Missing Robustness for Audio-Visual Speech Recognition</b></h2>
                <br/>

                <p align="center" class="center_text" id="authors">
                    <a target="_blank" href="https://hci.iwr.uni-heidelberg.de/vislearn/people">Fei Su</a><sup>1</sup>
                    &nbsp;&nbsp;
                    <a target="_blank" href="https://de.linkedin.com/in/malte-prinzler">Cancan Li</a><sup>2</sup>
                    &nbsp;&nbsp;
                    <a target="_blank" href="https://titus-leistner.de/pages/about-me.html">Juan Liu</a><sup>3</sup>
                    &nbsp;&nbsp;
                </p>

                <p class="center_text" align="center">
                    School of Computer Science 
                    Wuhan University      
                    
                </p>

                <br>

                <p class="center_text font-weight-bold" align="center">
                    
                </p>
                <br>

                <p align="center">
                    <a href="https://arxiv.org/pdf/2112.01554" target="__blank" class="btn btn-light">Paper</a>
                    <a href="https://github.com/philgras/neural-head-avatars" target="__blank" class="btn btn-dark">Code</a>
                </p>
                <br>

                <h3 class="w3-left-align" id="intro"><b>Introduction</b></h3>
                <p class="w3-justify">
                    Audio-Visual Speech Recognition (AVSR) leverages visual
                    information to enhance speech understanding in noisy en-
                    vironments. However, current models often assume stable,
                    frontal viewpoints, suffering significant performance drops
                    with non-frontal angles or when video signals are partially
                    missing. 1Our approach first employs a multi-view data gen-
                    eration strategy using 3D head avatar reconstruction, syn-
                    thesizing high-fidelity, viewpoint-diverse data to improve the
                    modelâ€™s ability to handle varying head poses. Then, we in-
                    troduce a self-supervised multi-view representation learning
                    model (MVL) , ensuring viewpoint-invariant and domain-
                    agnostic embeddings. Moreover, a Unified Modality Adapter
                    (UMA) module enables the model to gracefully revert to near
                    audio-only performance levels when visual inputs are unavail-
                    able, maintaining the benefits of audio-visual modeling. 
                </p>
                <br>

                <!-- <h3 class="w3-left-align" id="publication"><b>Preprint</b></h3>
                Paper - <a href="https://arxiv.org/pdf/ " target="__blank">ArXiv - pdf</a> (<a href="https://arxiv.org/abs/ " target="__blank">abs</a>)
                <center>
                    <a href="https://arxiv.org/pdf/ " target="__blank"><img src="preview.png" style="max-width:80%" /></a>
                </center><br> -->

                <h3 class="w3-left-align" id="video"><b>Video</b></h3>
                <p>
                    <iframe width="850" height="480" src="speaker01.mp4" frameborder="0"
                            allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                    <!-- <iframe width="850" height="480" src="https://www.youtube.com/" frameborder="0"
                        allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>-->
                <p/>
                <h3 class="w3-left-align" id="cite"><b>Cite</b></h3>
                <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 12px">
                
            @inproceedings{,
                title={Enhanced Self-Supervised Multi-View Representations With Modality-Missing Robustness for Audio-Visual Speech Recognition},
                author={},
                booktitle={on},
                pages={},
                year={}
            }
            
                </pre>
            </div>
        </div>
        <br/>
        <br/>
    </body>
</html>
