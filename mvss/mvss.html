<!DOCTYPE html>
<html>
    <head>
        <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
        <title></title>
        <link rel="stylesheet" href="../w3.css">
        <link rel="stylesheet"
              href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.0/css/bootstrap.min.css"
              integrity="sha384-9aIt2nRpC12Uk9gS9baDl411NQApFmC26EwAOH8WgZl5MYYxFfc+NcPb1dKGj7Sk"
              crossorigin="anonymous">
        <meta name="keywords" content="audio-visual speech recognition, self-supervised, multi-view representations, modality-missing, paper, research"/>
    </head>
    <body>
        <br/>
        <br/>
        <div class="w3-container" id="paper">
            <div class="w3-content" style="max-width:850px">
                <h2 align="center" id="title"><b>Enhanced Self-Supervised Multi-View Representations With Modality-Missing Robustness for Audio-Visual Speech Recognition</b></h2>
                <br/>

                <p align="center" class="center_text" id="authors">
                    <span>Anonymous submission</span>
                </p>

                <!-- <p class="center_text" align="center">
                    ///     
                    
                </p> -->

                <br>

                <p class="center_text font-weight-bold" align="center">
                    
                </p>
                <br>

                <p align="center">
                    <a href="paper.pdf" target="__blank" class="btn btn-light">Paper</a>
                    <a href="https://github.com/yakumostudio/MVL-UMA" target="__blank" class="btn btn-dark">Code</a>
                </p>
                <br>

                <h3 class="w3-left-align" id="intro"><b>Introduction</b></h3>
                <p class="w3-justify">
                    Audio-Visual Speech Recognition (AVSR) leverages visual
                    information to enhance speech understanding in noisy en
                    vironments. However, current models often assume stable,
                    frontal viewpoints, suffering significant performance drops
                    with non-frontal angles or when video signals are partially
                    missing. Our approach first employs a multi-view data generation strategy using 3D head avatar reconstruction, synthesizing high-fidelity, viewpoint-diverse data to improve the
                    modelâ€™s ability to handle varying head poses. Then, we in
                    troduce a self-supervised multi-view representation learning
                    model (MVL) , ensuring viewpoint-invariant and domain-
                    agnostic embeddings. Moreover, a Unified Modality Adapter
                    (UMA) model enables the model to gracefully revert to near
                    audio-only performance levels when visual inputs are unavail
                    able, maintaining the benefits of audio-visual modeling. 
                </p>
                <br>

                <!-- <h3 class="w3-left-align" id="publication"><b>Preprint</b></h3>
                Paper - <a href="https://arxiv.org/pdf/ " target="__blank">ArXiv - pdf</a> (<a href="https://arxiv.org/abs/ " target="__blank">abs</a>)
                <center>
                    <a href="https://arxiv.org/pdf/ " target="__blank"><img src="preview.png" style="max-width:80%" /></a>
                </center><br> -->

                <h3 class="w3-left-align" id="video"><b>Videos</b></h3>
                <div class="video-container">
                    <!-- Row 1: Speaker 01 -->
                    <div style="display: flex; justify-content: space-around; margin-bottom: 20px;">
                        <div style="text-align: center;">
                            <iframe src="speaker01_a0_e0.mp4" frameborder="0" style="width: 300px; height: 300px;"
                                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br>
                            <span>speaker01 (raw video)</span>
                        </div>
                        <div style="text-align: center;">
                            <iframe src="speaker01_a0_e-15.mp4" frameborder="0" style="width: 300px; height: 300px;"
                                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br>
                            <span>speaker01 (yaw: 0&#176;, pitch: -15&#176;)</span>
                        </div>
                        <div style="text-align: center;">
                            <iframe src="speaker01_a5_e15.mp4" frameborder="0" style="width: 300px; height: 300px;"
                                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br>
                            <span>speaker01 (yaw: 5&#176;, pitch: 15&#176;)</span>
                        </div>
                    </div>                 

                    <!-- Row 2: Speaker 02 -->
                    <div style="display: flex; justify-content: space-around; margin-bottom: 20px;">
                        <div style="text-align: center;">
                            <iframe src="speaker02_a0_e0.mp4" frameborder="0" style="width: 300px; height: 300px;"
                                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br>
                            <span>speaker02 (raw video)</span>
                        </div>
                        <div style="text-align: center;">
                            <iframe src="speaker02_a0_e-3.mp4" frameborder="0" style="width: 300px; height: 300px;"
                                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br>
                            <span>speaker02 (yaw: 0&#176;, pitch: -3&#176;)</span>
                        </div>
                        <div style="text-align: center;">
                            <iframe src="speaker02_a5_e3.mp4" frameborder="0" style="width: 300px; height: 300px;"
                                    allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
                            <br>
                            <span>speaker02 (yaw: 5&#176;, pitch: 3&#176;)</span>
                        </div>
                    </div>
                </div>


            <pre class="w3-panel w3-leftbar w3-light-grey" style="white-space: pre-wrap; font-family: monospace; font-size: 12px">    
            @inproceedings{,
                title={Enhanced Self-Supervised Multi-View Representations With Modality-Missing Robustness for Audio-Visual Speech Recognition},
                author={},
                booktitle={},
                pages={},
                year={}
            }           
            </pre>
            </div>
        </div>
        <br/>
        <br/>
    </body>
</html>
